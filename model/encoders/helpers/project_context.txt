Dataset description:
- 2004 video-text samples
- Arabic Seq2Seq Lip-reading/Visual-speech-recognitoin on Token/Character-Like-Level
- The tokens are: "Vocabulary mapping: {'ٱ': 1, 'يْ': 2, 'يّْ': 3, 'يِّ': 4, 'يُّ': 5, 'يَّ': 6, 'يٌّ': 7, 'يِ': 8, 'يُ': 9, 'يَ': 10, 'يٌ': 11, 'ي': 12, 'ى': 13, 'وْ': 14, 'وِّ': 15, 'وُّ': 16, 'وَّ': 17, 'وِ': 18, 'وُ': 19, 'وَ': 20, 'وً': 21, 'و': 22, 'هْ': 23, 'هُّ': 24, 'هِ': 25, 'هُ': 26, 'هَ': 27, 'نۢ': 28, 'نْ': 29, 'نِّ': 30, 'نُّ': 31, 'نَّ': 32, 'نِ': 33, 'نُ': 34, 'نَ': 35, 'مْ': 36, 'مّْ': 37, 'مِّ': 38, 'مُّ': 39, 'مَّ': 40, 'مِ': 41, 'مُ': 42, 'مَ': 43, 'مٍ': 44, 'مٌ': 45, 'مً': 46, 'لْ': 47, 'لّْ': 48, 'لِّ': 49, 'لُّ': 50, 'لَّ': 51, 'لِ': 52, 'لُ': 53, 'لَ': 54, 'لٍ': 55, 'لٌ': 56, 'لً': 57, 'كْ': 58, 'كِّ': 59, 'كَّ': 60, 'كِ': 61, 'كُ': 62, 'كَ': 63, 'قْ': 64, 'قَّ': 65, 'قِ': 66, 'قُ': 67, 'قَ': 68, 'قٍ': 69, 'قً': 70, 'فْ': 71, 'فِّ': 72, 'فَّ': 73, 'فِ': 74, 'فُ': 75, 'فَ': 76, 'غْ': 77, 'غِ': 78, 'غَ': 79, 'عْ': 80, 'عَّ': 81, 'عِ': 82, 'عُ': 83, 'عَ': 84, 'عٍ': 85, 'ظْ': 86, 'ظِّ': 87, 'ظَّ': 88, 'ظِ': 89, 'ظُ': 90, 'ظَ': 91, 'طْ': 92, 'طِّ': 93, 'طَّ': 94, 'طِ': 95, 'طُ': 96, 'طَ': 97, 'ضْ': 98, 'ضِّ': 99, 'ضُّ': 100, 'ضَّ': 101, 'ضِ': 102, 'ضُ': 103, 'ضَ': 104, 'ضً': 105, 'صْ': 106, 'صّْ': 107, 'صِّ': 108, 'صُّ': 109, 'صَّ': 110, 'صِ': 111, 'صُ': 112, 'صَ': 113, 'صٍ': 114, 'صً': 115, 'شْ': 116, 'شِّ': 117, 'شُّ': 118, 'شَّ': 119, 'شِ': 120, 'شُ': 121, 'شَ': 122, 'سْ': 123, 'سّْ': 124, 'سِّ': 125, 'سُّ': 126, 'سَّ': 127, 'سِ': 128, 'سُ': 129, 'سَ': 130, 'سٍ': 131, 'زْ': 132, 'زَّ': 133, 'زِ': 134, 'زُ': 135, 'زَ': 136, 'رْ': 137, 'رِّ': 138, 'رُّ': 139, 'رَّ': 140, 'رِ': 141, 'رُ': 142, 'رَ': 143, 'رٍ': 144, 'رٌ': 145, 'رً': 146, 'ذْ': 147, 'ذَّ': 148, 'ذِ': 149, 'ذُ': 150, 'ذَ': 151, 'دْ': 152, 'دِّ': 153, 'دُّ': 154, 'دَّ': 155, 'دًّ': 156, 'دِ': 157, 'دُ': 158, 'دَ': 159, 'دٍ': 160, 'دٌ': 161, 'دً': 162, 'خْ': 163, 'خِ': 164, 'خُ': 165, 'خَ': 166, 'حْ': 167, 'حَّ': 168, 'حِ': 169, 'حُ': 170, 'حَ': 171, 'جْ': 172, 'جِّ': 173, 'جُّ': 174, 'جَّ': 175, 'جِ': 176, 'جُ': 177, 'جَ': 178, 'ثْ': 179, 'ثِّ': 180, 'ثُّ': 181, 'ثَّ': 182, 'ثِ': 183, 'ثُ': 184, 'ثَ': 185, 'تْ': 186, 'تِّ': 187, 'تُّ': 188, 'تَّ': 189, 'تِ': 190, 'تُ': 191, 'تَ': 192, 'تٍ': 193, 'تٌ': 194, 'ةْ': 195, 'ةِ': 196, 'ةُ': 197, 'ةَ': 198, 'ةٍ': 199, 'ةٌ': 200, 'ةً': 201, 'بْ': 202, 'بِّ': 203, 'بَّ': 204, 'بِ': 205, 'بُ': 206, 'بَ': 207, 'بٍ': 208, 'بً': 209, 'ا': 210, 'ئْ': 211, 'ئِ': 212, 'ئَ': 213, 'ئً': 214, 'إِ': 215, 'ؤْ': 216, 'ؤُ': 217, 'ؤَ': 218, 'أْ': 219, 'أُ': 220, 'أَ': 221, 'آ': 222, 'ءْ': 223, 'ءِ': 224, 'ءَ': 225, 'ءً': 226}" 

Frequency Histogram in the 2004 sample dataset: "
ا: 2346
ٱ: 1999
لْ: 1409
ي: 1256
لَ: 821
مَ: 727
رَ: 710
هْ: 578
و: 562
عَ: 519
نْ: 469
أَ: 463
مُ: 435
وَ: 432
مِ: 413
قَ: 411
لِ: 411
إِ: 366
بِ: 346
يّْ: 346
رِ: 334
مْ: 333
يَّ: 320
جَ: 317
نَ: 316
فِ: 315
يَ: 297
دَ: 281
تْ: 269
ةِ: 267
تِ: 264
رْ: 263
بَ: 260
حَ: 259
تَ: 255
نِ: 249
بْ: 224
قُ: 224
سْ: 222
دِ: 215
ئِ: 201
عْ: 197
وْ: 194
فَ: 185
ى: 181
زِ: 175
يْ: 170
دْ: 166
طَ: 163
نَّ: 162
سِ: 160
كَ: 155
شَ: 154
وَّ: 150
صَ: 148
رُ: 135
لُ: 132
عِ: 129
خَ: 128
سُ: 128
قِ: 119
سَ: 119
حْ: 118
دَّ: 116
ةُ: 116
كُ: 111
ءْ: 109
هَ: 108
كِ: 106
جِ: 101
تَّ: 99
زَ: 98
جْ: 96
أُ: 94
سُّ: 94
لَّ: 87
حُ: 86
قْ: 85
فْ: 81
صِ: 81
حِ: 81
خْ: 80
ةْ: 76
رَّ: 76
وِ: 74
تُ: 69
زْ: 66
هُ: 65
شَّ: 65
دُ: 61
شْ: 60
دِّ: 60
غَ: 59
عُ: 58
ثِ: 57
ظَ: 56
وُ: 55
نُ: 54
سَّ: 54
مَّ: 51
ضَ: 47
لِّ: 47
ظِ: 46
ثُ: 40
هِ: 40
نۢ: 40
ذَ: 38
يُّ: 38
يِّ: 37
تُّ: 35
أْ: 34
ثَ: 34
صْ: 33
جُ: 33
تٍ: 32
تِّ: 30
ذِ: 29
ءِ: 29
نِّ: 27
صً: 27
يُ: 26
ئَ: 26
طْ: 25
شِ: 24
رُّ: 24
لً: 23
طِ: 21
بُ: 20
شُ: 19
خِ: 19
كْ: 18
ضِ: 18
كَّ: 17
ةَ: 16
رِّ: 15
ةٌ: 15
شِّ: 14
صَّ: 13
قٍ: 12
ضْ: 11
ءَ: 11
وِّ: 11
ثْ: 10
صُ: 10
غْ: 10
لُّ: 10
يِ: 9
فُ: 8
طَّ: 8
دٌ: 8
ةٍ: 8
ذْ: 7
دٍ: 7
سِّ: 6
زُ: 6
خُ: 6
تٌ: 6
شُّ: 5
طُ: 5
صِّ: 5
ظَّ: 4
ؤْ: 4
حَّ: 4
مِّ: 4
قَّ: 4
صُّ: 4
ةً: 4
نُّ: 4
ضَّ: 3
ظْ: 3
دً: 3
ذُ: 3
ؤُ: 3
ضً: 3
فَّ: 3
ثَّ: 3
رً: 3
عٍ: 3
صٍ: 3
جِّ: 2
مُّ: 2
مً: 2
ؤَ: 2
جَّ: 2
ثِّ: 2
جُّ: 2
بَّ: 2
بً: 2
ثُّ: 2
مٍ: 2
ذَّ: 2
ءً: 2
ضُ: 2
بِّ: 2
لّْ: 2
بٍ: 2
هُّ: 1
مّْ: 1
كِّ: 1
سّْ: 1
عَّ: 1
غِ: 1
دُّ: 1
سٍ: 1
يٌ: 1
ظِّ: 1
قً: 1
ضِّ: 1
فِّ: 1
طِّ: 1
لٍ: 1
دًّ: 1
مٌ: 1
رٍ: 1
رٌ: 1
صّْ: 1
يٌّ: 1
ظُ: 1
زَّ: 1
ئً: 1
ضُّ: 1
وً: 1
ئْ: 1
لٌ: 1
وُّ: 1

"
- An example from the 2004 video-text samples CSVs : 
"
File: 00139.csv
word
قَالَ
إِنَّ
ٱلْإِنْتِخَابَا

File: 00154.csv
word
قَالَتْ
إِنَّ
مُوَاطِنْ

File: 00156.csv
word
قَدْ
قَالَ
إِنَّ
إِسْتِعَادَةْ
...
"

- The single label token is a diacritized character 
- videos are 1.2 seconds, about 2 to 6 words each, 25 FPS
- max video frames length = 38 and min: 29 and avg: 30.84, max label sequence length = 24 and min = 6 and avg: 14.19
- Our current preprocessed videos are saved in gray 112*112 done with dlib's 68 point face detector using 48-68 point landmarks
- in my python file master_all_avsr.py, Follow the repo's (mpc001-txt and the actual code files are in auto_avsr folder in workspace) approach in terms of training and modeling, specifically in using the transformer decoder and the beam search

Can help (mitigations):
- Data augmentations (with Kornia):
    train:
        random_crop:
            size: 88
        normalize:
            mean: 0.419232189655303955078125
            std: 0.133925855159759521484375
        random_horizontal_flip:
            probability: 0.5

        time_warp?
        AdaptiveTimeMask? (with word tokens version)

    val/test:
        central_crop:
            size: 88
        normalize:
            mean: 0.419232189655303955078125
            std: 0.133925855159759521484375

- Hyperparameter tuning
    - Using their WarmupCosineScheduler - AdamW with (Betas: (0.9, 0.98), weight_decay: 0.01, eps: 1e-9)
    - tuning parameters in each model options and training setup parameters
    - wandb sweeping

- Maybe (not now): Unfreeze pretrained visual-frontend
- Maybe but dangerous and not recommended (May affect dataset characteristics badly): class/token balance (reduce Alef and Lam wights cuz they appear ultimately a lot)
- We are using fine grained tokens instead of tokenization because we are making a small-scale experiments as a simple product or actually Graduation Project also because there's no time, 
- we are training on Single P100 GPU on kaggle.com





1. Plot and compare learning curves
• Log/train and val losses and CER side by side each epoch (you already print CER). Generate a quick matplotlib plot of
– training loss vs epochs
– validation CER vs epochs
This will immediately show whether you’re over‐fitting, under‐fitting, or simply stuck.

2. Check over- vs under-fitting
• If training CER stays high → model capacity or optimization issues.
• If training CER is low but validation CER is high → overfitting.
• If both are high but don’t improve → representational bottleneck or bad optimization.

3. Inspect gradient flow
• Instrument your training loop to record gradient norms on key layers each epoch—particularly:
– the adapter/proj_encoder
– the last layer(s) of your frozen frontend
If gradients into the frontend are zeroed (when TRAIN_FRONTEND=False) or extremely small even when unfrozen, your learning rate or scheduler may be preventing it from adapting.

4. Evaluate pure CTC decoding
• Bypass the transformer decoder and run CTC‐only greedy decoding on validation. If CTC‐only CER is similarly poor, the problem lies upstream in feature or encoder representation.

5. Qualitative error analysis
• Print out the top-k worst‐CER samples each epoch, side by side:
– frame length, target text, predicted text, scores
Sometimes you’ll spot a pattern (e.g. certain visemes always get confused).

6. Confusion statistics
• Build a token‐level confusion matrix over your validation set. Which characters does the model most often swap? This can reveal, for example, that certain diacritics or shapes never get learned.

7. Hyperparameter sanity check
• Make sure your learning rates, scheduler, weight decay, and loss weights (CTC vs Attention) are reasonable. Perhaps your decoder is dominating the loss and “oversmoothed” its outputs.

8. Quick “sanity” model
• Temporarily replace the visual frontend with random noise or a linear projection. See if the model can still overfit on a tiny subset (say, 10 videos). If not, there’s a bug in your training loop.